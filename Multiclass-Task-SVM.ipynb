{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Task Notebook\n",
    "### Contains Code for Binary task SVM Model creation, training and validation(hyperparameter tuning)\n",
    "### The multiclass targets are arranged as<br/>no_tumor = 0<br/>glioma_tumor = 1<br/>meningioma_tumor = 2<br/>pituitary_tumor = 3<br/>Respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#tqdm is for progress bar functionality in code, must be installed for code to function (TO DO: include exception if tqdm not imported )\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Importing libraries used for SVM classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "#Importing functions notebook containing functions created to streamline code\n",
    "from ipynb.fs.full.functions import load_dataset, dataset_PCA, Tuned_SVM_train, SVM_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Dataset and doing final preprocessing\n",
    "#### We Load the preprocessed data and carry out PCA on the image array here for multi-classification training and test data. <br/> Initial data preprocessing code will be very similar to Binary Task SVM as the only difference is the use of the multiclass label file instead of binary.\n",
    "## 1.1 Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets successfully loaded with shapes:\n",
      "Y Shape:\n",
      "(3000,)\n",
      "X Shape:\n",
      "(3000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Calls load_dataset function from \"functions.ipynb\" which loads the X data and Y label datasets for Multiclass task from the inputted file paths\n",
    "#It prints the loaded array shapes to verify it has completed properly\n",
    "X, Y = load_dataset('.\\dataset\\Image_DF_Flat.pkl', './dataset/Y_Multiclass_label.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can do PCA for the images but must be done separately for binary and multiclass task as the data must be split first<br/> This is because we must do PCA on the training data only (fit and transform it) and then only use the transform on the test data to prevent bias<br/> We select 400 components as it provides around 96% explained variance as shown previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Splitting data in to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing(70% training and 30% testing data)\n",
    "xTrain,xTest,yTrain,yTest=train_test_split(X, Y, train_size = 0.7)\n",
    "\n",
    "#Rescaling the dataframe as the pixel values range from 0 to 255\n",
    "#We want it to be between 0 to 1 to let it pass through the NN and models\n",
    "xTrain_Scaled = xTrain/255\n",
    "xTest_Scaled = xTest/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA conducted with 400 components.\n",
      "The percentage of Explained Variance of the dataset from PCA is: 96.51006820948666\n"
     ]
    }
   ],
   "source": [
    "#Initialising PCA with 400 components determined in preprocessing notebook\n",
    "#Calls the dataset_PCA function defined in \"functions.ipynb\" to carry out PCa\n",
    "#Input arguements are number of components, xTrain data and xTest data\n",
    "#We put in the scaled X train and test data\n",
    "xTrain_PCA, xTest_transformed = dataset_PCA(400, xTrain_Scaled, xTest_Scaled)\n",
    "\n",
    "#Function returns the resultant explained variance percentage when we use 400 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Building\n",
    "\n",
    "## 2.1 SVM without tuning\n",
    "### 2.1.1 Training<br/> We first train SVM without hyperparameter tuning to assess the performance of the model with the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training model using SVC without hyperparameter tuning\n",
    "multiclass_SVM_untuned = SVC(probability = True)\n",
    "multiclass_SVM_untuned.fit(xTrain_PCA, yTrain.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Obtaining the Prediction results and performance<br/> Showing the confusion matrix and general assessment metrics using the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Results for SVM are:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.75      0.80       126\n",
      "         1.0       0.79      0.78      0.79       274\n",
      "         2.0       0.75      0.76      0.75       251\n",
      "         3.0       0.90      0.95      0.92       249\n",
      "\n",
      "    accuracy                           0.82       900\n",
      "   macro avg       0.82      0.81      0.82       900\n",
      "weighted avg       0.82      0.82      0.82       900\n",
      "\n",
      "The confusion matrix is:\n",
      "[[ 95   9  17   5]\n",
      " [  6 215  45   8]\n",
      " [  7  40 190  14]\n",
      " [  3   7   3 236]]\n"
     ]
    }
   ],
   "source": [
    "#Calls SVM_predictions function from \"functions.ipynb\" to carry out the predictions using the untuned multiclass SVM model we made.\n",
    "#It prints out the classification report of the predictions as well as the confusion matrix\n",
    "#Returns the predictions\n",
    "multiclass_SVM_untuned_pred = SVM_predictions(multiclass_SVM_untuned, xTest_transformed, yTest) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SVM with validation\n",
    "\n",
    "### 2.2.1 Training<br/> We now use SVM again but with validation, tuning of the hyperparameter values<br/> This is done using an exhaustive Gridsearch of the parameter values given. <br/> After model is trained, we obtain the best parameters found by the gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SVM but this time with gridsearch to tune hyperparameter\n",
    "\n",
    "#Define the parameter ranges\n",
    "#We test various values of:\n",
    "# C\n",
    "# gamma\n",
    "# Type of Kernel to use\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Tuned SVM Model successfully trained and tuned\n",
      "The best hyperparameters found by gridsearch are:\n",
      "{'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "The new model created after hyperparameter tuning is:\n",
      "SVC(C=1000, gamma=0.1, probability=True)\n"
     ]
    }
   ],
   "source": [
    "#Calls Tuned_SVM_train from \"functions.ipynb\" to conduct training and tuning of SVM model using gridsearch\n",
    "#Full details on the input arguements listed in the functions notebook\n",
    "\n",
    "#Function prints the resultant best hyperparameters found and new details of the model\n",
    "#Returns the tuned model\n",
    "multiclass_SVM_Tuned = Tuned_SVM_train(param_grid, 5, xTrain_PCA, yTrain, True)\n",
    "\n",
    "#est time = 11mins 10s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Obtaining the Prediction results and performance<br/> Showing the confusion matrix and general assessment metrics using the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Results for SVM are:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.71      0.78       126\n",
      "         1.0       0.88      0.82      0.85       274\n",
      "         2.0       0.76      0.86      0.81       251\n",
      "         3.0       0.92      0.96      0.94       249\n",
      "\n",
      "    accuracy                           0.85       900\n",
      "   macro avg       0.86      0.84      0.85       900\n",
      "weighted avg       0.86      0.85      0.85       900\n",
      "\n",
      "The confusion matrix is:\n",
      "[[ 89   5  27   5]\n",
      " [  5 226  39   4]\n",
      " [  4  21 215  11]\n",
      " [  3   5   2 239]]\n"
     ]
    }
   ],
   "source": [
    "#Calls SVM_predictions function from \"functions.ipynb\"\n",
    "#This time we are doing predictions with the tuned SVM model\n",
    "multiclass_SVM_Tuned_pred = SVM_predictions(multiclass_SVM_Tuned , xTest_transformed, yTest)\n",
    "\n",
    "#It prints out the classification report of the predictions as well as the confusion matrix\n",
    "#Returns the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving tuned and base SVM models\n",
    "save_path = \"./Models/Multiclassification\"\n",
    "Tuned_SVM_filename = 'Tuned_multiclass_SVM.sav'\n",
    "Base_SVM_filename = 'Untuned_multiclass_SVM.sav'\n",
    "\n",
    "#Using Pickle to put them in files\n",
    "pkl.dump(multiclass_SVM_untuned , open(os.path.join(save_path, Base_SVM_filename), 'wb'))\n",
    "pkl.dump(multiclass_SVM_Tuned, open(os.path.join(save_path, Tuned_SVM_filename), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "290ec9070849e8daf2fe4bd922bd966932468e9fb905fb4b926b0f4d9e51cc32"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('gym': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
