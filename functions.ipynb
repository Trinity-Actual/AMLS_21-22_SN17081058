{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from numpy import expand_dims\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#tqdm is for progress bar functionality in code, must be installed for code to function\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Importing libraries used for SVM classification and model assessment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "#Libraries for CNN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which takes in file path and image size arguements and extracts the images from the given path and resizes them to the determined size (square dimensions)\n",
    "def image_array_resize(file_path,IMG_SIZE):\n",
    "    #Initialises empty list for population through for loop\n",
    "    image_data = []\n",
    "\n",
    "    #For loop will run through all items in the directory listed, in this case the image folder containing all 3000 mri images in our dataset.\n",
    "    #x in this case will print out the full filename (in this case the name of each IMAGE_xxxx.jpg), therefore we can use it to generate paths to \n",
    "    #All the images via the for loop.\n",
    "\n",
    "    #tqdm just gives a progress bar for image extraction process\n",
    "    #listdir counts total number of items in the designated folder, in this case the image folder with all the original images\n",
    "    for x in tqdm(os.listdir(file_path)):\n",
    "        #Creates path to images per iteration\n",
    "        image_path = os.path.join(file_path,x)\n",
    "\n",
    "        #Reads the corresponding image using cv2.imread\n",
    "        file_array = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        #Resizes images based on the value of IMG_SIZE\n",
    "        resize_file_array = cv2.resize(file_array, (IMG_SIZE,IMG_SIZE))\n",
    "        \n",
    "        #Stores the image array information into the image_data list created. Append makes sure it adds information in order.\n",
    "        image_data.append(resize_file_array)\n",
    "\n",
    "        \n",
    "        #CODE FOR DEBUGGING\n",
    "        #print(x)\n",
    "        #plt.imshow(file_array, cmap='gray')  #graph it\n",
    "        #plt.show()  #display! \n",
    "        #break\n",
    "\n",
    "    #Converts list into numpy array for easier processing\n",
    "    image_data_array = np.array(image_data)   \n",
    "    print(\"Successfully extracted original Images from dataset!\")\n",
    "    return image_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset function\n",
    "def load_dataset(data_file_path, label_file_path):\n",
    "    #Reading created pkl files for labels and image data using file path inputs\n",
    "    Target_labels = pd.read_pickle(label_file_path)\n",
    "    Data_samples = pd.read_pickle(data_file_path)\n",
    "\n",
    "    #Turning it into X and Y arrays, X being the input data and Y being the corresponding labels\n",
    "    X = Data_samples\n",
    "    #Taking just the label portion for editing into our Target Y array\n",
    "    Y = Target_labels.iloc[:, 0]\n",
    "\n",
    "\n",
    "    #For Display, Returns the shape of the resultant arrays for reference and confirmation it worked\n",
    "    #In coursework should return Y of 3000 labels and X with 3000 samples with 784 features\n",
    "    print(\"Datasets successfully loaded with shapes:\")\n",
    "    print(\"Y Shape:\")\n",
    "    print(Y.shape)\n",
    "    print(\"X Shape:\")\n",
    "    print(X.shape)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Dataset for CNN\n",
    "\n",
    "def load_dataset_CNN(data_file_path, label_file_path):\n",
    "    #Reading created pkl files for labels and image data using file path inputs\n",
    "    Target_labels = pd.read_pickle(label_file_path)\n",
    "    MRI_2D_imgs = pd.read_pickle(data_file_path)\n",
    "\n",
    "    #Reshaping the 2D_MRI array images back into 3D Array\n",
    "    MRI_2D_img_array = np.array(MRI_2D_imgs)\n",
    "    MRI_img_array = MRI_2D_img_array.reshape(MRI_2D_img_array.shape[0],50,50)\n",
    "\n",
    "    #Adding 4th channel to array (for convnet fitting)\n",
    "    #The last channel is indicating whether it is a RGB channel (3) or grayscale (1) image\n",
    "    MRI_img_array_channel = MRI_img_array.reshape(3000,50,50,1)\n",
    "\n",
    "    #Getting our Y and X inputs for the model and scaling the X inputs\n",
    "    #Carrying out scaling of the pixel data per element so that it is between 0 and 1\n",
    "    X = MRI_img_array_channel/255\n",
    "    #Taking just the label portion for editing into our Target Y array\n",
    "    Y = Target_labels.iloc[:, 0]\n",
    "\n",
    "    \n",
    "    #For Display, Returns the shape of the resultant arrays for reference and confirmation it worked\n",
    "    #In coursework should return Y of 3000 labels and X with 4 Dimensions: 3000, 100, 100, 1\n",
    "    #Verifying the new array is 4D\n",
    "    print(\"Datasets successfully loaded with shapes:\")\n",
    "   \n",
    "    print(\"X Shape:\")\n",
    "    print(X.shape)\n",
    "\n",
    "    print(\"Y Shape:\")\n",
    "    print(Y.shape)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "\n",
    "def dataset_PCA(n_components, xTrain, xTest):\n",
    "\n",
    "    #Initialising PCA with the target number of components from input\n",
    "    n_PCA = PCA(n_components= n_components)\n",
    "\n",
    "    #Fitting and Transforming training dataset provided, this prevents model from learning about test data statistics\n",
    "    xTrain_PCA = n_PCA.fit_transform(xTrain)\n",
    "\n",
    "    #Transforming the test dataset provided\n",
    "    #We only transform test dataset as we do not want the model to learn about the test data statistics\n",
    "    xTest_transformed = n_PCA.transform(xTest)\n",
    "\n",
    "    #Prints the percentage of explained variance to verify it is greater than our threshold of 95%\n",
    "    #print(np.cumsum(n_PCA.explained_variance_ratio_ * 100)[-1])\n",
    "\n",
    "    #Function complete message\n",
    "    #Shows number of components for PCA \n",
    "    print(\"PCA conducted with \" + str(n_components) + \" components.\")\n",
    "\n",
    "    #Prints the percentage of explained variance to verify it is greater than our threshold of 95%\n",
    "    print(\"The percentage of Explained Variance of the dataset from PCA is: \" + str(np.cumsum(n_PCA.explained_variance_ratio_ * 100)[-1]))\n",
    "\n",
    "    return xTrain_PCA, xTest_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned SVM training\n",
    "#Function which fits SVM model using gridsearch based on parameter grid inputted, it also returns the best hyperparameters found during gridsearch and the details of the new model\n",
    "# Input Arguements:\n",
    "# param_grid = parameter grid containing hyperparameter values to do a gridsearch with\n",
    "# cv = number of cross validation folds\n",
    "# xTrain = X training data\n",
    "# yTrain = Y training data labels\n",
    "# probability = whether we want to enable probability estimates, True for multiclass, false for binary\n",
    "def Tuned_SVM_train(param_grid, cv, xTrain, yTrain, probability):\n",
    "\n",
    "    #Initialises new SVM model which will conduct a gridsearch through the given parameters\n",
    "    SVM_grid = GridSearchCV(SVC(probability = probability), param_grid, refit = True, verbose = 1, cv = cv)\n",
    "\n",
    "    #Fitting model with grid search based on our training dataset\n",
    "    SVM_grid.fit(xTrain, yTrain.values.ravel())\n",
    "\n",
    "    #Confirmation Message\n",
    "    print(\"Tuned SVM Model successfully trained and tuned\")\n",
    "\n",
    "    #Display the best parameters after the hyperparameter tuning\n",
    "    print(\"The best hyperparameters found by gridsearch are:\")\n",
    "    print(SVM_grid.best_params_)\n",
    "\n",
    "    #Print the new details of the SVM model after tuning\n",
    "    print(\"The new model created after hyperparameter tuning is:\")\n",
    "    print(SVM_grid.best_estimator_)\n",
    "\n",
    "    return SVM_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM predictions\n",
    "#Function which takes in SVM model and test datasets and runs predictions. \n",
    "#Returns the performance of the model with the classification_report, confusion matrix as well as the predictions of the model\n",
    "\n",
    "def SVM_predictions(model, xTest, yTest):\n",
    "    #Printing prediction results\n",
    "    SVM_pred = model.predict(xTest)\n",
    "    print(\"The Results for SVM are:\")\n",
    "    print(classification_report(yTest, SVM_pred))\n",
    "\n",
    "    #Printing the confusion matrix for SVM\n",
    "    print(\"The confusion matrix is:\")\n",
    "    print(confusion_matrix(yTest, SVM_pred))\n",
    "\n",
    "    return SVM_pred\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "290ec9070849e8daf2fe4bd922bd966932468e9fb905fb4b926b0f4d9e51cc32"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('gym': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
